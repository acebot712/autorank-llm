{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d7c1ac9-8580-4525-97c0-ddb530eb4a98",
   "metadata": {},
   "source": [
    "1. Using LLM for now.\n",
    "2. Replace with vLLM when you have compute and GPUs\n",
    "3. Use Ollama in local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef140c-a6b1-4172-89a8-b01aa8c592e6",
   "metadata": {},
   "source": [
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5378b4-bea0-4182-8c17-ce6bbc88bec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # CPU\n",
    "# %%timeit -n 3 -r 3\n",
    "# import llm\n",
    "\n",
    "# model = llm.get_model(\"mistral-7b-instruct-v0\")\n",
    "# response = model.prompt(\"3 names for a pet cow\",stream=False)\n",
    "# print(response.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443b3fe-6b42-40dc-bdcc-79d1de91ec77",
   "metadata": {},
   "source": [
    "vLLM:- [vLLM](https://github.com/vllm-project/vllm/pull/1901)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483ca0b-d035-4924-8e77-533aa63662eb",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b07953e-6dd5-4a43-90f3-c0586d547438",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral:instruct\")\n",
    "response = llm(\"3 names for a pet cow\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c332c6-37c0-45fa-8f6a-0e34a51a5a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama2\")\n",
    "response = llm(\"3 names for a pet cow\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d654a-753f-44af-8c37-bc9d8db87c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"orca-mini\")\n",
    "response = llm(\"3 names for a pet cow\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313e59f6-21bd-4102-af85-d98713a7bced",
   "metadata": {},
   "source": [
    "# Step 2: Conceptualizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66848554-356f-436d-93c2-eedd1b48641d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluator.model_name='mistral:instruct',evaluatee.model_name='orca-mini'\n",
      "response=' 1. Bessie\\n2. Daisy\\n3. Rosie'\n",
      "\n",
      "I would rate this response a 7 out of 10. The names suggested are typical and common for pets, but they are relevant to the task of naming a pet cow. However, the response could have been more creative or unique, which would have made it more memorable and interesting.\n",
      "matches=['7', '10']\n",
      "evaluator.model_name='mistral:instruct',evaluatee.model_name='llama2'\n",
      "response='\\nSure, here are three names for a pet cow:\\n\\n1. Bessie - a classic and timeless name for a pet cow.\\n2. Daisy - a sweet and playful name that evokes images of green pastures and sunny days.\\n3. Buttercup - a charming and affectionate name that is perfect for a friendly and loving pet cow.'\n",
      "\n",
      "I would give the response a score of 7 out of 10. While the names suggested are certainly appropriate for pet cows, the response is brief and doesn't provide much additional context or information. Additionally, there could be other factors to consider when choosing a name for a pet cow, such as breed, personality traits, and personal preferences.\n",
      "matches=['7', '10']\n",
      "evaluator.model_name='orca-mini',evaluatee.model_name='mistral:instruct'\n",
      "response='1. Moo-Moo\\n2. Bessie\\n3. Daisy'\n",
      " I would rate the response a 10, as it was very relevant and provided three unique and appropriate names for a pet cow. The names were also easy to understand and relate to the theme of a pet cow.\n",
      "matches=['10']\n",
      "evaluator.model_name='orca-mini',evaluatee.model_name='llama2'\n",
      "response='\\nSure, here are three names for a pet cow:\\n\\n1. Bessie - a classic name that has been popular for pet cows for many years. It\\'s simple and easy to pronounce, and it has a friendly and approachable sound to it.\\n2. Daisy - a sweet and charming name that is perfect for a gentle and affectionate cow. It evokes images of a bright and sunny day in the countryside, and it\\'s a name that is easy to associate with warmth and happiness.\\n3. Buttercup - a name that is both playful and endearing. It has a fun and whimsical sound to it, and it\\'s a great choice for a pet cow that is friendly and outgoing. The word \"buttercup\" also has connotations of warmth and happiness, which could be fitting for a pet cow that is loving and affectionate.'\n",
      " I would rate this response a 9 out of 10. The names provided are all appropriate and fitting for a pet cow, and they have been well-thought-out with consideration to their relevance and quality.\n",
      "matches=['9', '10']\n",
      "evaluator.model_name='llama2',evaluatee.model_name='mistral:instruct'\n",
      "response='1. Moo Moo\\n2. Bessie\\n3. Daisy'\n",
      "\n",
      "I would rate the response as follows:\n",
      "\n",
      "1. Moo Moo - 8/10 (Relevant and funny, but not particularly original or creative)\n",
      "2. Bessie - 9/10 (A classic and timeless name for a pet cow, easy to pronounce and remember)\n",
      "3. Daisy - 7/10 (A nice, simple name that fits well with the theme of a pet cow, but may not be as memorable or distinctive as some other options)\n",
      "\n",
      "Overall, I would give the response a score of 8 out of 10 for its relevance and quality.\n",
      "matches=['1', '8', '10', '2', '9', '10', '3', '7', '10', '8', '10']\n",
      "evaluator.model_name='llama2',evaluatee.model_name='orca-mini'\n",
      "response=' 1. Cowlette\\n2. Moo-chow\\n3. Cowsan'\n",
      "\n",
      "I would rate the response as follows:\n",
      "\n",
      "1. Cowlette - 8/10 (Relevance: 9/10, Quality: 8/10)\n",
      "This name is a play on the word \"cow\" and \"couture\", which is a clever and creative combination. It has a nice ring to it and is easy to remember.\n",
      "2. Moo-chow - 6/10 (Relevance: 7/10, Quality: 6/10)\n",
      "While the name \"Moo-chow\" is cute and catchy, it may not be as memorable or unique as other options. The use of \"moo\" and \"chow\" together may also feel a bit forced or contrived.\n",
      "3. Cowsan - 4/10 (Relevance: 5/10, Quality: 4/10)\n",
      "The name \"Cowsan\" is not as creative or memorable as the other options. It does not have the same level of cleverness or playfulness as \"Cowlette\".\n",
      "\n",
      "Overall, I would give the response a score of 6/10 for relevance and 5/10 for quality.\n",
      "matches=['1', '8', '10', '9', '10', '8', '10', '2', '6', '10', '7', '10', '6', '10', '3', '4', '10', '5', '10', '4', '10', '6', '10', '5', '10']\n",
      "mistral:instruct final skill level: 481.8780868302066\n",
      "orca-mini final skill level: 578.2637598110337\n",
      "llama2 final skill level: 600.9017818974926\n"
     ]
    }
   ],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.llms import Ollama\n",
    "import re\n",
    "import random\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, name, model_name):\n",
    "        self.name = name\n",
    "        self.model_name = model_name\n",
    "        self.skill_level = random.uniform(50, 100)  # Random initial skill level\n",
    "        self.evaluations = []\n",
    "\n",
    "    def perform_task(self, task):\n",
    "        # The evaluatee performs the task\n",
    "        llm = Ollama(model=self.model_name)\n",
    "        response = llm(task)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def evaluate(self, original_task, task_response):\n",
    "        # The evaluator scores the task response\n",
    "        scoring_prompt = f\"Task: '{original_task}'. Response: '{task_response}'. Please rate the response numerically based on its relevance and quality. Give a score between 0 and 10 exclusive of 0 and 10 itself.\"\n",
    "        llm = Ollama(model=self.model_name)\n",
    "        score_response = llm(scoring_prompt)\n",
    "        print(f\"{score_response}\")\n",
    "        return self.extract_numerical_score(score_response)\n",
    "\n",
    "    def extract_numerical_score(self, response_text):\n",
    "        # Extract numbers between 1 and 10 and calculate average\n",
    "        matches = re.findall(r'\\b(?:[1-9](?:\\.\\d+)?|10)\\b', response_text)\n",
    "        print(f\"{matches=}\")\n",
    "        if matches:\n",
    "            scores = [int(match) for match in matches]\n",
    "            average_score = sum(scores) / len(scores)\n",
    "            return average_score\n",
    "        else:\n",
    "            return 0  # Default to 0 if no matches found\n",
    "\n",
    "    def update_skill_level(self):\n",
    "        # Update skill level based on evaluations\n",
    "        if self.evaluations:\n",
    "            new_skill_level = sum(self.evaluations) / len(self.evaluations)\n",
    "            change_in_skill_level = abs(new_skill_level - self.skill_level)\n",
    "            self.skill_level = new_skill_level\n",
    "            self.evaluations = []  # Reset for next round\n",
    "            return change_in_skill_level\n",
    "        return 0\n",
    "\n",
    "def normalize_skill_levels(llms):\n",
    "    # Normalize skill levels to a range of 0 to 100\n",
    "    skill_levels = [llm.skill_level for llm in llms]\n",
    "    min_skill = min(skill_levels)\n",
    "    max_skill = max(skill_levels)\n",
    "    for llm in llms:\n",
    "        llm.skill_level = 100 * (llm.skill_level - min_skill) / (max_skill - min_skill) if max_skill != min_skill else 50\n",
    "\n",
    "# Model names\n",
    "model_names = [\"mistral:instruct\", \"orca-mini\", \"llama2\"]\n",
    "\n",
    "# Create LLM instances\n",
    "llms = [LLM(f\"LLM{i}\", model_name) for i, model_name in enumerate(model_names, start=1)]\n",
    "\n",
    "# Evaluation task\n",
    "task = \"3 names for a pet cow\"\n",
    "\n",
    "# Convergence threshold\n",
    "threshold = 0.5\n",
    "converged = False\n",
    "\n",
    "while not converged:\n",
    "    max_change = 0\n",
    "    for evaluator in llms:\n",
    "        for evaluatee in llms:\n",
    "            if evaluator != evaluatee:\n",
    "                print(f\"{evaluator.model_name=},{evaluatee.model_name=}\")\n",
    "                task_response = evaluatee.perform_task(task)\n",
    "                score = evaluator.evaluate(task, task_response)\n",
    "                weighted_score = score * evaluator.skill_level\n",
    "                evaluatee.evaluations.append(weighted_score)\n",
    "\n",
    "    # Normalize skill levels\n",
    "    normalize_skill_levels(llms)\n",
    "\n",
    "    # Check for convergence\n",
    "    changes = [llm.update_skill_level() for llm in llms]\n",
    "    max_change = max(changes)\n",
    "    converged = max_change < threshold\n",
    "    converged = True # for debugging purpose\n",
    "\n",
    "# Print final skill levels\n",
    "for llm in llms:\n",
    "    print(f\"{llm.model_name} final skill level: {llm.skill_level}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84485b1e-052a-47a4-8eff-21ee24ccbf71",
   "metadata": {},
   "source": [
    "# Production Quality Refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677fc38-b8f2-4762-b396-cd5bb3aeffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from langchain.llms import Ollama\n",
    "import logging\n",
    "\n",
    "# Set up basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class LLM:\n",
    "    def __init__(self, name, model_name):\n",
    "        self.name = name\n",
    "        self.model_name = model_name\n",
    "        self.skill_level = random.uniform(50, 100)\n",
    "        self.evaluations = []\n",
    "\n",
    "    def perform_task(self, task):\n",
    "        try:\n",
    "            llm = Ollama(model=self.model_name)\n",
    "            response = llm(task)\n",
    "            logging.info(f\"Response from {self.name}: {response}\")\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in perform_task for {self.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate(self, original_task, task_response):\n",
    "        try:\n",
    "            scoring_prompt = f\"Task: '{original_task}'. Response: '{task_response}'. Rate the response numerically between 1 and 9.\"\n",
    "            llm = Ollama(model=self.model_name)\n",
    "            score_response = llm(scoring_prompt)\n",
    "            logging.info(f\"Evaluation by {self.name}: {score_response}\")\n",
    "            return self.extract_numerical_score(score_response)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in evaluate for {self.name}: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def extract_numerical_score(self, response_text):\n",
    "        try:\n",
    "            matches = re.findall(r'\\b(?:[1-9](?:\\.\\d+)?|10)\\b', response_text)\n",
    "            if matches:\n",
    "                scores = [float(match) for match in matches]\n",
    "                average_score = sum(scores) / len(scores)\n",
    "                return average_score\n",
    "            else:\n",
    "                return 0\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in extract_numerical_score: {e}\")\n",
    "            return 0\n",
    "\n",
    "    def update_skill_level(self):\n",
    "        try:\n",
    "            if self.evaluations:\n",
    "                new_skill_level = sum(self.evaluations) / len(self.evaluations)\n",
    "                change_in_skill_level = abs(new_skill_level - self.skill_level)\n",
    "                self.skill_level = new_skill_level\n",
    "                self.evaluations = []\n",
    "                return change_in_skill_level\n",
    "            return 0\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in update_skill_level for {self.name}: {e}\")\n",
    "            return 0\n",
    "\n",
    "def normalize_skill_levels(llms):\n",
    "    try:\n",
    "        skill_levels = [llm.skill_level for llm in llms]\n",
    "        min_skill = min(skill_levels)\n",
    "        max_skill = max(skill_levels)\n",
    "        for llm in llms:\n",
    "            llm.skill_level = 100 * (llm.skill_level - min_skill) / (max_skill - min_skill) if max_skill != min_skill else 50\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error in normalize_skill_levels: {e}\")\n",
    "\n",
    "def rank_llms(llms):\n",
    "    sorted_llms = sorted(llms, key=lambda x: x.skill_level, reverse=True)\n",
    "    logging.info(\"Final Ranking of LLMs:\")\n",
    "    for rank, llm in enumerate(sorted_llms, start=1):\n",
    "        logging.info(f\"Rank {rank}: {llm.name} with skill level {llm.skill_level}\")\n",
    "\n",
    "# Model names\n",
    "model_names = [\"mistral:instruct\", \"orca-mini\", \"llama2\"]\n",
    "\n",
    "# Create LLM instances\n",
    "llms = [LLM(f\"LLM{i}\", model_name) for i, model_name in enumerate(model_names, start=1)]\n",
    "\n",
    "# Evaluation task\n",
    "task = \"3 names for a pet cow\"\n",
    "\n",
    "# Convergence threshold\n",
    "threshold = 0.5\n",
    "converged = False\n",
    "\n",
    "while not converged:\n",
    "    max_change = 0\n",
    "    for evaluator in llms:\n",
    "        for evaluatee in llms:\n",
    "            if evaluator != evaluatee:\n",
    "                task_response = evaluatee.perform_task(task)\n",
    "                if task_response:\n",
    "                    score = evaluator.evaluate(task, task_response)\n",
    "                    weighted_score = score * evaluator.skill_level\n",
    "                    evaluatee.evaluations.append(weighted_score)\n",
    "\n",
    "    # Normalize skill levels\n",
    "    normalize_skill_levels(llms)\n",
    "\n",
    "    # Check for convergence\n",
    "    changes = [llm.update_skill_level() for llm in llms]\n",
    "    max_change = max(changes)\n",
    "    converged = max_change < threshold\n",
    "\n",
    "# Rank the LLMs based on their final skill levels\n",
    "rank_llms(llms)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
